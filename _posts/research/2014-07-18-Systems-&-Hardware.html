---
layout: post
img: hardware.png
tag: arch
summary: building hardware for systems and building systems in hardware
---



<div class="row-fluid">
<i>
<p>
<!--With the end of Dennard Scaling and the slowing of Moore's law,-->
Systems and hardware closely interact with each other.
WukLab is taking non-traditional approaches to "building systems" and "building hardware"
in response to new hardware and software trends.
We build systems on new types of hardware like non-volatile memory;
we design new hardware architectures for new software trends;
we build systems like OSes in hardware;
and we improve hardware experiences with software flexibility.
</p>
</i>
</div>

<hr>
<hr>


	<div class="row-fluid">
	  <h3>Virtualized FPGA Cloud Services</h3>
          <div class="span6">
        	<p class="text-left">

We are building a virtualization layer on FPGA to improve the flexibility, programmability,
utilization, and safety of FPGA acceleration as a cloud service.
		</p>
          <p>
Stay tuned for more details.
		</p>
	  </div>
	</div>

      <hr>

	<div class="row-fluid">
	  <h3>Hardware-Based Disaggregated Memory Services</h3>
          <div class="span6">
        	<p class="text-left">

We are building a hardware platform which provides disaggregated and distributed memory services
to both existing datacenter servers and disaggregated processors.
Our hardware-based disaggregated memory services can be accessed by virtual memory interface,
key-value store interface, and other customized interfaces.
		</p>
          <p>
Stay tuned for more details.
		</p>
	  </div>
	</div>

      <hr>

	<div class="row-fluid">
	  <h3>Disaggregated Persistent Memory</h3>
          <div class="span6">
        	<p class="text-left">
One viable approach to deploy persistent memory (PM) in datacenters
is to attach PM as self-contained devices to the network as disaggregated
persistent memory, or <b>DPM</b>. DPM requires no changes
to existing servers in datacenters; without the need to
include a processor, DPM devices are cheap to build;
and by sharing DPM across compute servers, they offer
great elasticity and efficient resource packing.
</p>
<p>
We propose three architectures of DPM: 1) compute nodes
directly access DPM (DPM-Direct); 2) compute nodes
send requests to a coordinator server, which then accesses
DPM to complete a request (DPM-Central); and
3) compute nodes directly access DPM for data operations
and communicate with a global metadata server for
the control plane (DPM-Sep). Based on these architectures,
we built three atomic, crash-consistent data stores.
</p>
	  </div>
<!--
          <div class="span5">
<p class="text-center">
<img width="300" src="img/research/DPM.jpg" class="center">
</p>
	  </div>
-->
	</div>

      <hr>

	<div class="row-fluid">
	  <h3>Distributed Shared Persistent Memory</h3>
          <div class="span6">
<!--
<div class="row">

<div class="span3 offset1">
<p>
<img width="120" src="img/research/hotpot-logo.jpg">
</p>
</div>
<div class="span8">
-->
        	<p class="text-left">
NVMs have the potential to greatly
improve the performance and reliability of large-scale
applications in datacenters.
However, it is still unclear how to best utilize them
in distributed, datacenter environments.
</p>
<!--
</div>
</div>
-->
<p>
We introduce Distributed Shared Persistent Memory
(DSPM), a new framework for using persistent memories
in distributed datacenter environments. DSPM provides
a new abstraction that allows applications to both perform
traditional memory load and store instructions and
to name, share, and persist their data. We built <strong>Hotpot</strong>,
a kernel-level DSPM system that provides low-latency,
transparent memory accesses, data persistence, data reliability,
and high availability.
		</p>
          <p>
<strong>Get Hotpot <a target="_blank" href="https://github.com/WukLab/Hotpot">here</a>.</strong>
          </p>
	  </div>
	  <!--
          <div class="span5">
<img width="400" src="img/research/hotpot.jpg">
	  </div>
	  -->
	</div>

<hr>

	<div class="row-fluid">
    <h3>Reliable and Highly-Available NVMM</h3>
          <div class="span6">
        	<p class="text-left">
	  <!--
    <div class="w-100">
        <img class="float-right m-2" height="200" src="img/research/Mojim_achitecture.jpg">
	  -->
        <p>
        NVMM would be especially useful in large-scale data center environments,
        where reliability and availability are critical.
        However, providing reliability and availability to NVMM is challenging,
        since the latency of data replication can squander the low latency that NVMM can provide.
        </p>
        <p>
        <strong>Mojim</strong> is a system that provides the reliability and availability that large-scale storage systems require,
        while preserving the performance of NVMM.
        Mojim achieves these goals by using a two-tier architecture in which the primary tier contains
        a mirrored pair of nodes
        and the secondary tier contains one or more secondary backup nodes with weakly consistent copies of data.
        Mojim uses highly-optimized replication protocols, software, and networking stacks.
        Our evaluation results show that surprisingly Mojim provides replicated NVMM with similar or even better performance than un-replicated NVMM
        (reducing latency by 27% to 63% and delivering between 0.4 and 2.7X the throughput).
        Mojim also outperforms MongoDB's replication system by 3.4 to 4X.
        </p>
    </div>
</div>

<hr>

